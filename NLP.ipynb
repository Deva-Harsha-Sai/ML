{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f7ysaSKvBt2",
        "outputId": "594f4bbe-3f6b-4c84-ecd6-cb6b782b8ba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'yourself', 'nor', 'have', 'further', 'is', 'only', \"mustn't\", 'more', 'does', 'just', \"you're\", 'him', 'any', \"wasn't\", 'am', 'i', 'out', 'in', 'don', 'such', \"should've\", 'above', 'mightn', 'our', 'the', 'itself', 'why', 'when', \"you've\", 'than', 'how', 'under', \"won't\", 'had', 'whom', 'too', 'their', 'until', \"don't\", 'you', 'll', 'isn', 'hasn', 'having', 'here', 'those', 'no', 'not', 'will', 'herself', 'couldn', 'between', 'so', \"she's\", \"isn't\", 'needn', 'shouldn', 've', 'that', 'other', 'haven', 'she', \"hadn't\", 'because', \"mightn't\", 'shan', 'an', 'which', 'then', 'mustn', 'his', \"it's\", 'myself', 'hadn', 'after', 'some', 'or', 'yourselves', 'at', 's', 'your', \"hasn't\", 'theirs', \"didn't\", 'what', 'same', 'being', 'own', 'me', \"needn't\", 'its', 'do', 'up', 'down', 'off', \"you'll\", 'very', \"shouldn't\", 'were', 're', \"that'll\", \"weren't\", 'ma', 'again', 'from', 'and', 'ours', 'was', 'd', 'm', 'been', 'now', 'he', 'should', 'both', 'few', 'during', 'over', 'my', 'who', \"you'd\", 'while', 'they', 'through', 'did', 'her', 'about', 'to', 'all', \"aren't\", 'didn', 'with', 'weren', 'there', 'where', 'once', 'themselves', 'are', 'doing', 'himself', \"haven't\", 'but', 'has', 'hers', 'by', 'before', 'o', \"couldn't\", 'doesn', 'yours', 'for', 'most', 'as', 'wasn', 'them', 'each', 'this', 'of', 'ain', 'y', 'below', 'on', 'a', 'we', 'against', 'ourselves', \"wouldn't\", 'be', 'can', \"shan't\", 'if', 'aren', 'it', 'won', 'into', 'wouldn', 't', 'these', \"doesn't\"}\n"
          ]
        }
      ],
      "source": [
        "#1..\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = input(\"Enter a sentence for performing tokenization and stopword removal :  \").lower()\n",
        "tokens = word_tokenize(text)\n",
        "#filtered_tokens = [w for w in tokens if not w in stop_words]\n",
        "#print(filtered_tokens)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uH93sDrkvQYi",
        "outputId": "c84384cc-9866-44ac-c2ce-a86331c672c4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence for performing tokenization and stopword removal :  Tokenization refers to a process by which a piece of sensitive data, such as a credit card number, is replaced by a surrogate value known as a token. The sensitive data still generally needs to be stored securely at one centralized location for subsequent reference and requires strong protections around it.\n",
            "['tokenization', 'refers', 'to', 'a', 'process', 'by', 'which', 'a', 'piece', 'of', 'sensitive', 'data', ',', 'such', 'as', 'a', 'credit', 'card', 'number', ',', 'is', 'replaced', 'by', 'a', 'surrogate', 'value', 'known', 'as', 'a', 'token', '.', 'the', 'sensitive', 'data', 'still', 'generally', 'needs', 'to', 'be', 'stored', 'securely', 'at', 'one', 'centralized', 'location', 'for', 'subsequent', 'reference', 'and', 'requires', 'strong', 'protections', 'around', 'it', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens = [w for w in tokens if w not in stop_words]"
      ],
      "metadata": {
        "id": "eBMMxAvlxDxP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SfoqIi6xkbk",
        "outputId": "1cf60544-78cd-4c08-ae81-148d8ce525e1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tokenization', 'refers', 'process', 'piece', 'sensitive', 'data', ',', 'credit', 'card', 'number', ',', 'replaced', 'surrogate', 'value', 'known', 'token', '.', 'sensitive', 'data', 'still', 'generally', 'needs', 'stored', 'securely', 'one', 'centralized', 'location', 'subsequent', 'reference', 'requires', 'strong', 'protections', 'around', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.. Lemmatization using porter stemmer\n",
        "#This is a shortcut,write the entire code for taking input and removing stopwords in observation\n",
        "from nltk.stem import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "lemmatized_tokens = [porter_stemmer.stem(token) for token in filtered_tokens]\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cW6h9jnXxsUK",
        "outputId": "fef94e9f-d5b8-4e2d-bb03-af18ac51c172"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['token', 'refer', 'process', 'piec', 'sensit', 'data', ',', 'credit', 'card', 'number', ',', 'replac', 'surrog', 'valu', 'known', 'token', '.', 'sensit', 'data', 'still', 'gener', 'need', 'store', 'secur', 'one', 'central', 'locat', 'subsequ', 'refer', 'requir', 'strong', 'protect', 'around', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Write a python program for word analysis and word generation\n",
        "text= input(\"Please enter a sentence\")\n",
        "words = text.split()\n",
        "word_count = {}\n",
        "for word in words:\n",
        "    if word in word_count:\n",
        "        word_count[word] += 1\n",
        "    else:\n",
        "        word_count[word] = 1\n",
        "print(word_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX9i1SoK1VCQ",
        "outputId": "e3da56e5-b2ad-4a7d-da12-171467ae1146"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter a sentenceTokenization refers to a process by which a piece of sensitive data, such as a credit card number, is replaced by a surrogate value known as a token. The sensitive data still generally needs to be stored securely at one centralized location for subsequent reference and requires strong protections around it.\n",
            "{'Tokenization': 1, 'refers': 1, 'to': 2, 'a': 5, 'process': 1, 'by': 2, 'which': 1, 'piece': 1, 'of': 1, 'sensitive': 2, 'data,': 1, 'such': 1, 'as': 2, 'credit': 1, 'card': 1, 'number,': 1, 'is': 1, 'replaced': 1, 'surrogate': 1, 'value': 1, 'known': 1, 'token.': 1, 'The': 1, 'data': 1, 'still': 1, 'generally': 1, 'needs': 1, 'be': 1, 'stored': 1, 'securely': 1, 'at': 1, 'one': 1, 'centralized': 1, 'location': 1, 'for': 1, 'subsequent': 1, 'reference': 1, 'and': 1, 'requires': 1, 'strong': 1, 'protections': 1, 'around': 1, 'it.': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3b- Word generation\n",
        "import random\n",
        "sample_words = [\"apple\",\"banana\", \"cherry\"]\n",
        "random_word = random.choice(sample_words)\n",
        "print(random_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdP2sWOQ4VRu",
        "outputId": "d393ec4c-8bba-47ec-d16d-c892a3be183f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cherry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5th  --> Various stemming methods\n",
        "text =input(\"Enter the text : \")\n",
        "\n",
        "#Porter Stemmer\n",
        "porter = PorterStemmer()\n",
        "tokens = text.split()\n",
        "stemmed_tokens = [porter.stem(token) for token in tokens]\n",
        "print(\"With porter stemmer :\",stemmed_tokens)\n",
        "\n",
        "#Snowball Stemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "tokens = text.split()\n",
        "stemmed_tokens = [snowball.stem(token) for token in tokens]\n",
        "print(\"With snowball stemmer :\",stemmed_tokens)\n",
        "\n",
        "\n",
        "#Lancaster Stemmer\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "lancaster = LancasterStemmer()\n",
        "tokens = text.split()\n",
        "stemmed_tokens = [lancaster.stem(token) for token in tokens]\n",
        "print(\"With lancaster stemmer :\",stemmed_tokens)\n",
        "\n",
        "\n",
        "#Regexp Stemmer\n",
        "from nltk.stem.regexp import RegexpStemmer\n",
        "regexp = RegexpStemmer(\"ing\")\n",
        "tokens = text.split()\n",
        "stemmed_tokens = [regexp.stem(token) for token in tokens]\n",
        "print(\"With regexp stemmer :\",stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA0eT8gZ6-lV",
        "outputId": "8d364db5-25b2-4739-c5d0-dbae1b498165"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text : Tokenization is the process of replacing sensitive data with unique identification symbols that retain all the essential information about the data withou\n",
            "With porter stemmer : ['token', 'is', 'the', 'process', 'of', 'replac', 'sensit', 'data', 'with', 'uniqu', 'identif', 'symbol', 'that', 'retain', 'all', 'the', 'essenti', 'inform', 'about', 'the', 'data', 'withou']\n",
            "With snowball stemmer : ['token', 'is', 'the', 'process', 'of', 'replac', 'sensit', 'data', 'with', 'uniqu', 'identif', 'symbol', 'that', 'retain', 'all', 'the', 'essenti', 'inform', 'about', 'the', 'data', 'withou']\n",
            "With lancaster stemmer : ['tok', 'is', 'the', 'process', 'of', 'replac', 'sensit', 'dat', 'with', 'un', 'id', 'symbol', 'that', 'retain', 'al', 'the', 'ess', 'inform', 'about', 'the', 'dat', 'withou']\n",
            "With regexp stemmer : ['Tokenization', 'is', 'the', 'process', 'of', 'replac', 'sensitive', 'data', 'with', 'unique', 'identification', 'symbols', 'that', 'retain', 'all', 'the', 'essential', 'information', 'about', 'the', 'data', 'withou']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Parts of speech\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZxsvkZV8sId",
        "outputId": "be75f2ae-de00-4658-c96a-9ae515eb3dff"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('hello', 'NN'), ('hyderabad', 'NN'), ('cat', 'NN'), ('dogs', 'NNS'), ('care', 'NN'), ('car', 'NN'), ('speed', 'NN')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokens = [\"hyderabad\",\"is\",\"a\",\"cool\",\"city\",\"with\",\"all\",\"facilities\"]\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdE307nSAHhY",
        "outputId": "8532d0df-a968-4223-af89-df949b3142c3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('hyderabad', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('cool', 'JJ'), ('city', 'NN'), ('with', 'IN'), ('all', 'DT'), ('facilities', 'NNS')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in pos_tags:\n",
        "    print(\"word is: \",i[0],end = \" \")\n",
        "    print(\" --> pos tag is: \",i[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUvyacV5AvlI",
        "outputId": "0b21078d-01b8-4d6f-9601-04255e4391bf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word is:  hyderabad  --> pos tag is:  NN\n",
            "word is:  is  --> pos tag is:  VBZ\n",
            "word is:  a  --> pos tag is:  DT\n",
            "word is:  cool  --> pos tag is:  JJ\n",
            "word is:  city  --> pos tag is:  NN\n",
            "word is:  with  --> pos tag is:  IN\n",
            "word is:  all  --> pos tag is:  DT\n",
            "word is:  facilities  --> pos tag is:  NNS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "xmEXaO6pCeuX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}